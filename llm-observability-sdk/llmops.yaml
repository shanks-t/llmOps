# LLM Ops SDK Configuration
#
# This configuration file is used by llmops.init() to set up auto-instrumentation.
# Place this file in your project root or specify the path explicitly.
#
# Usage:
#   import llmops
#   llmops.init()  # Uses ./llmops.yaml by default
#   llmops.init(config_path="/path/to/llmops.yaml")  # Custom path

# Service identification
service:
  name: "llmops-demo"
  version: "1.0.0"

# Backend selection: "phoenix" or "mlflow"
# Can be overridden via LLMOPS_BACKEND environment variable
backend: phoenix

# Phoenix backend configuration (OpenInference semantic conventions)
# Phoenix provides rich LLM-specific observability with specialized UI
phoenix:
  endpoint: http://localhost:6006/v1/traces
  # project_name: my-project  # Optional: organize traces by project

# MLflow backend configuration (native autolog)
# MLflow integrates with ML experiment tracking and model registry
mlflow:
  tracking_uri: http://localhost:5001
  # experiment_name: llmops-demo  # Optional: defaults to service.name

# Privacy settings
privacy:
  # Capture input/output content in traces
  # Set to false in production to avoid logging sensitive data
  capture_content: true

# Auto-instrumentation settings
auto_instrumentation:
  enabled: true
  # Optionally disable specific instrumentors
  # disabled:
  #   - langchain
  #   - llamaindex
